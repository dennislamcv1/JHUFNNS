{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2ad4f82",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"JHU.png\" width=\"200\" alt=\"Johns Hopkins University logo\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0415d39",
   "metadata": {},
   "source": [
    "# Hands-on Lab: Exploring Batch Normalization in Deep Neural Networks\n",
    "\n",
    "Estimated time needed: **60** minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd5a77a",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "\n",
    "Batch normalization is a crucial technique in deep learning that accelerates training and improves the performance of deep neural networks. This lab provides a hands-on experience implementing batch normalization in a deep learning model using popular frameworks like TensorFlow or PyTorch. You will train a model on a dataset with and without batch normalization, comparing their training convergence speeds and generalization performances.\n",
    "\n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "\n",
    "- Understand the concept and benefits of batch normalization.\n",
    "- Implement batch normalization in a deep neural network.\n",
    "- Compare training performance with and without batch normalization.\n",
    "- Visualize and interpret the results to understand its impact on model convergence and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12076a2a",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "We will use the Fashion MNIST dataset, a popular dataset comprising 60,000 training images and 10,000 testing images of grayscale images representing 10 fashion categories (e.g., T-shirts, sneakers, bags). Each image is 28x28 pixels.\n",
    "\n",
    "**Dataset Details**:\n",
    "- Input features: 28x28 grayscale images (flattened to 784 features).\n",
    "- Output labels: 10 classes, e.g., T-shirt/top, trouser, pullover.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cff208",
   "metadata": {},
   "source": [
    "## Assignment Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2010445b",
   "metadata": {},
   "source": [
    "### Step 1: Install and Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147b0e24",
   "metadata": {},
   "source": [
    "> **Note**: Please ignore any warnings that appear during execution; they will not affect the correctness of your code or the upcoming tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a52b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries (if not already installed)\n",
    "!pip install tensorflow numpy matplotlib\n",
    "\n",
    "# Import required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd26fa1",
   "metadata": {},
   "source": [
    "### Task 2: Data Preprocessing and Visualization for Fashion MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7ad4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Fashion MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize the data to the range [0, 1]\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# Flatten the data\n",
    "# Write your code here!\n",
    "\n",
    "\n",
    "# Display a sample image\n",
    "# Write your code here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9278c6c1",
   "metadata": {},
   "source": [
    "**Explanation**: \n",
    "\n",
    "- **Load the Fashion MNIST Dataset**: This step loads the training and test datasets, including both the images and their associated class labels.\n",
    "- **Normalize the Data to the Range [0, 1]**: The pixel values are scaled from [0, 255] to [0, 1] to improve the performance and stability of the model during training.\n",
    "- **Flatten the Data**: The 28x28 pixel images are reshaped into 1D vectors of size 784, making them suitable for use in machine learning models.\n",
    "- **Display a Sample Image**: A sample image from the training set is displayed with its class label for visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3c55f0",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here to view/hide the solution.</summary>\n",
    "    \n",
    "```python\n",
    "\n",
    "# Flatten the data\n",
    "X_train = X_train.reshape(-1, 28 * 28)\n",
    "X_test = X_test.reshape(-1, 28 * 28)\n",
    "\n",
    "# Display a sample image\n",
    "plt.imshow(X_train[0].reshape(28, 28), cmap=\"gray\")\n",
    "plt.title(f\"Class Label: {y_train[0]}\")\n",
    "plt.show()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b416ce5",
   "metadata": {},
   "source": [
    "## Task 3: Define and Train the Model Without Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664f9b8f",
   "metadata": {},
   "source": [
    "> **Note**:\n",
    "The code provided defines and trains a neural network model without batch normalization. Depending on the computational resources and dataset size, training the model may take some time. Be patient, as training deep learning models can be time-intensive, especially without optimizations like batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b354af67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define the model without batch normalization\n",
    "model_without_bn = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(784,)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "# Write your code here!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "# Write your code here!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970c529d",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "In this step, you build a basic neural network without batch normalization. This model serves as the baseline for comparison, allowing us to observe how training progresses without normalization of activations between layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e062d3",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here to view/hide the solution.</summary>\n",
    "    \n",
    "```python\n",
    "    \n",
    "# Compile the model\n",
    "model_without_bn.compile(optimizer='adam',\n",
    "                         loss='sparse_categorical_crossentropy',\n",
    "                         metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_without_bn = model_without_bn.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "    \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bee64b",
   "metadata": {},
   "source": [
    "### Task 4: Define and Train the Model With Batch Normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4dbe0b",
   "metadata": {},
   "source": [
    "> **Note**:\n",
    "This code compiles the model using the Adam optimizer and sparse categorical cross-entropy loss, then trains it for 10 epochs with a batch size of 32. The training may take some time depending on your system's resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4427f64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Define the model with batch normalization\n",
    "model_with_bn = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(784,)),\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "# Write your code here!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "# Write your code here!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a204ebf0",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "Batch normalization layers are added to the architecture. These layers normalize the activations of each layer, reducing internal covariate shift and potentially improving training speed and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ea1c75",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here to view/hide the solution.</summary>\n",
    "    \n",
    "```python\n",
    "\n",
    "# Compile the model\n",
    "model_with_bn.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_with_bn = model_with_bn.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=32\n",
    ")\n",
    "    \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79496d4",
   "metadata": {},
   "source": [
    "### Task 5: Compare Training Convergence Speeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc903e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "# Write your code here!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197c1ead",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "The training loss of both models is plotted to compare convergence speeds. Batch normalization typically results in smoother loss curves and faster convergence, especially in deeper or more complex networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477c3236",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here to view/hide the solution.</summary>\n",
    "    \n",
    "```python\n",
    "    \n",
    "# Plot training loss\n",
    "plt.plot(history_without_bn.history['loss'], label='Without Batch Norm')\n",
    "plt.plot(history_with_bn.history['loss'], label='With Batch Norm')\n",
    "plt.title('Training Loss Comparison')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88eb79b",
   "metadata": {},
   "source": [
    "### Task 6: Evaluate Generalization Performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35be1b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both models on the test dataset\n",
    "test_loss_without_bn, test_acc_without_bn = model_without_bn.evaluate(X_test, y_test, verbose=0)\n",
    "test_loss_with_bn, test_acc_with_bn = model_with_bn.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the test accuracy for the model both with and without batch normalization for comparison\n",
    "# Write your code here!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458dd7d4",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here to view/hide the solution.</summary>\n",
    "    \n",
    "```python\n",
    "\n",
    "print(f\"Test Accuracy Without Batch Norm: {test_acc_without_bn:.4f}\")\n",
    "print(f\"Test Accuracy With Batch Norm: {test_acc_with_bn:.4f}\")\n",
    "    \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcecb6ea",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "The models are evaluated on the test set to measure their generalization ability. This step highlights whether batch normalization improves the model's performance on unseen data, focusing on stability and potential overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73896c40",
   "metadata": {},
   "source": [
    "### Explaination of outcome:\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Batch Normalization's Purpose**\n",
    "Start by reinforcing the **primary purpose** of batch normalization:\n",
    "- **Faster Training Convergence**: It helps models train faster by normalizing inputs to each layer.\n",
    "- **Regularization Effect**: It can reduce overfitting by adding noise to layer activations during training.\n",
    "\n",
    "Even if the final accuracies are similar, batch normalization might still have improved the **training process** (e.g., smoother convergence or reduced epochs to achieve similar accuracy).\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Analyze Training Behavior**\n",
    "Encourage you to compare **training curves** (loss/accuracy over epochs). Explain these possibilities:\n",
    "- **Convergence Speed**: Did the model with batch normalization converge faster?\n",
    "- **Smoothness**: Is the loss/accuracy curve smoother for the batch-normalized model, indicating more stable training?\n",
    "  \n",
    "If the curves are nearly identical, emphasize that **batch normalization's benefits are more apparent in deeper networks** or with more complex datasets.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Factors Influencing Batch Normalization's Impact**\n",
    "Why the results might be similar:\n",
    "- **Model Architecture**: The current model is relatively shallow. Batch normalization is more effective in **very deep networks**.\n",
    "- **Dataset Simplicity**: Fashion MNIST is a manageable dataset. Batch normalization's advantages become clearer with **more complex datasets** or highly imbalanced data.\n",
    "- **Optimizer**: Using Adam (a robust optimizer) might reduce the observable benefits of batch normalization because Adam already adapts learning rates effectively.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Highlight Generalization Insight**\n",
    "Why test accuracy might not differ much:\n",
    "- **Model Capacity**: Both models have sufficient capacity to learn Fashion MNIST. Adding batch normalization didn’t significantly alter this.\n",
    "- **Overfitting**: Batch normalization acts as a regularizer. However, the dataset may not be complex enough to showcase overfitting issues that batch normalization could mitigate.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Encourage Further Exploration**\n",
    "Opportunities to explore:\n",
    "1. **Increase Model Depth**: Add more layers and observe if batch normalization improves performance.\n",
    "2. **Use a Complex Dataset**: Try datasets complex datasets, where batch normalization often shows a greater effect.\n",
    "3. **Alter Hyperparameters**: Experiment with learning rates, optimizers, or dropout alongside batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095f7d14",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "- Batch normalization helps in faster convergence during training.\n",
    "- It improves generalization by reducing overfitting, especially in deeper networks.\n",
    "- Training with batch normalization results in more stable and consistent gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e5fec0",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "\n",
    "In this lab, you implemented and compared a deep neural network with and without batch normalization. By analyzing training losses and test accuracies, you observed the impact of batch normalization on convergence speed and generalization performance. This foundational knowledge is crucial for designing efficient and robust neural network architectures in real-world applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
