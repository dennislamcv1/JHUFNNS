{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2ad4f82",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"JHU.png\" width=\"200\" alt=\"Johns Hopkins University logo\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0415d39",
   "metadata": {},
   "source": [
    "# Hands-On Lab: Implementing a Simple Feedforward Neural Network on the Iris Dataset\n",
    "\n",
    "Estimated time needed: **60** minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd5a77a",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "\n",
    "In this hands-on lab, you will build a simple feedforward neural network to classify flowers in the Iris dataset into three categories: **Setosa**, **Versicolor**, and **Virginica**. The lab involves using Python libraries such as Keras, TensorFlow, and Scikit-learn for data preprocessing, model creation, training, and evaluation. \n",
    "\n",
    "You will explore how different optimization algorithms SGD, Momentum, and Adam impact the convergence rate and overall model performance. This lab is designed to provide practical experience with deep learning concepts and expose learners to essential machine learning workflows.\n",
    "\n",
    "\n",
    "## Objectives\n",
    "By the end of this lab, learners will be able to:\n",
    "1. Load and preprocess the Iris dataset, including feature scaling and target one-hot encoding.\n",
    "2. Implement a feedforward neural network using Keras or TensorFlow.\n",
    "3. Train and evaluate the neural network using different optimization algorithms:\n",
    "   - Stochastic Gradient Descent (SGD)\n",
    "   - SGD with Momentum\n",
    "   - Adam Optimizer\n",
    "4. Visualize and compare the performance of each optimizer using metrics like accuracy, precision, recall, and the confusion matrix.\n",
    "5. Understand the importance of preprocessing, network design, and hyperparameter tuning in building effective models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12076a2a",
   "metadata": {},
   "source": [
    "## Dataset Used\n",
    "\n",
    "**Iris Dataset**:  \n",
    "The Iris dataset is a widely used toy dataset for classification tasks. It contains 150 samples of iris flowers with the following characteristics:  \n",
    "- **Features (4)**: Sepal length, Sepal width, Petal length, Petal width.  \n",
    "- **Target Classes (3)**:  \n",
    "  - *Setosa* (label 0)  \n",
    "  - *Versicolor* (label 1)  \n",
    "  - *Virginica* (label 2)  \n",
    "\n",
    "Each sample belongs to one of these three categories. The dataset is simple and well-separated, making it ideal for experimenting with basic machine learning and neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cff208",
   "metadata": {},
   "source": [
    "## Implementation: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2010445b",
   "metadata": {},
   "source": [
    "### Step 1: Install and Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefbfec7",
   "metadata": {},
   "source": [
    "> **Note**: Please be patient while the required packages are being installed. This may take some time due to the multiple dependencies, but it should complete shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a52b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all necessary libraries\n",
    "!pip install numpy pandas matplotlib seaborn scikit-learn\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd26fa1",
   "metadata": {},
   "source": [
    "### Step 2: Load and Explore the Dataset\n",
    "\n",
    "**Explanation**: This code loads the Iris dataset and creates a DataFrame for easier analysis. The pairplot helps visualize relationships between features and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7ad4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(data=np.c_[iris['data'], iris['target']],\n",
    "                    columns=iris['feature_names'] + ['target'])\n",
    "\n",
    "# Display dataset information\n",
    "# Write your code here!\n",
    "\n",
    "\n",
    "# Visualize pairplot\n",
    "# Write your code here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3c55f0",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here to view/hide the solution.</summary>\n",
    "    \n",
    "```python\n",
    "\n",
    "# Display dataset information\n",
    "print(data.info())  # Shows data types and null values\n",
    "print(data.head())  # Displays the first 5 rows of the dataset\n",
    "\n",
    "# Visualize pairplot\n",
    "sns.pairplot(data, hue='target', diag_kind='kde', markers=[\"o\", \"s\", \"D\"])\n",
    "plt.show()  # Pairwise plots of features with species color-coded\n",
    "\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b416ce5",
   "metadata": {},
   "source": [
    "### Step 3:  Preprocess the Data\n",
    "\n",
    "**Explanation**: The data is split into training and testing sets. The target is converted to one-hot encoding (e.g., [1, 0, 0] for Setosa). Features are standardized for better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b354af67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Features and target\n",
    "X = iris['data']  # Feature matrix (sepal and petal measurements)\n",
    "y = iris['target'].reshape(-1, 1)  # Target values reshaped into column format\n",
    "\n",
    "# One-hot encode target\n",
    "# Write your code here!\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "# Write your code here!\n",
    "\n",
    "\n",
    "# Normalize features\n",
    "# Write your code here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e062d3",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here to view/hide the solution.</summary>\n",
    "    \n",
    "```python\n",
    "    \n",
    "# One-hot encode target\n",
    "encoder = OneHotEncoder(sparse_output=False)  # Use 'sparse_output' instead of 'sparse'\n",
    "y_encoded = encoder.fit_transform(y)  # Convert target to one-hot encoding\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)  # Standardize features for stability\n",
    "X_test = scaler.transform(X_test)\n",
    "    \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bee64b",
   "metadata": {},
   "source": [
    "### Step 4: Design the Feedforward Neural Network\n",
    "\n",
    "**Explanation**: The network has two hidden layers with ReLU activation and one output layer with softmax activation to predict probabilities for the three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4427f64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define model\n",
    "# Write your code here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ea1c75",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here to view/hide the solution.</summary>\n",
    "    \n",
    "```python\n",
    "\n",
    "# Define model\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Dense(10, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer + 1st hidden layer\n",
    "        Dense(8, activation='relu'),  # 2nd hidden layer\n",
    "        Dense(3, activation='softmax')  # Output layer with 3 classes\n",
    "    ])\n",
    "    return model\n",
    "    \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5871e1",
   "metadata": {},
   "source": [
    "### Step 5: Train the Model with Different Optimization Algorithms\n",
    "\n",
    "**Explanation**: The model is trained using different optimizers: SGD (basic gradient descent), Momentum (adds velocity), and Adam (adaptive learning rates). Training history is logged for each optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ce1d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import History\n",
    "\n",
    "# Train model with different optimizers\n",
    "optimizers = {'SGD': SGD(learning_rate=0.01), 'Momentum': SGD(learning_rate=0.01, momentum=0.9), 'Adam': Adam(learning_rate=0.01)}\n",
    "histories = {}\n",
    "\n",
    "# Loop through each optimizer, train the model, evaluate its performance, and store the training history for comparison.\n",
    "# Write your code here!\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2094245e",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here to view/hide the solution.</summary>\n",
    "    \n",
    "```python\n",
    "    \n",
    "# Loop through each optimizer, train the model, evaluate its performance, and store the training history for comparison.\n",
    "    \n",
    "for opt_name, optimizer in optimizers.items():\n",
    "    print(f\"\\nTraining with {opt_name} optimizer...\")\n",
    "    model = create_model()\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])  # Compile model\n",
    "    history = model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), batch_size=8, verbose=0)  # Train\n",
    "    histories[opt_name] = history.history  # Store training history\n",
    "    print(f\"Final Test Accuracy with {opt_name}: {model.evaluate(X_test, y_test, verbose=0)[1]:.4f}\")\n",
    "    \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe931c2",
   "metadata": {},
   "source": [
    "### Step 6: Visualize and Compare Convergence Rates\n",
    "\n",
    "**Explanation**: Loss and accuracy curves are plotted for each optimizer, showing how fast they converge and their final performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af17892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss and accuracy for each optimizer\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss\n",
    "# Write your code here!\n",
    "\n",
    "\n",
    "\n",
    "# Accuracy\n",
    "# Write your code here!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a839720",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here to view/hide the solution.</summary>\n",
    "    \n",
    "```python\n",
    "    \n",
    "# Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "for opt_name, history in histories.items():\n",
    "    plt.plot(history['loss'], label=f'{opt_name} Loss')  # Plot loss over epochs\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "for opt_name, history in histories.items():\n",
    "    plt.plot(history['accuracy'], label=f'{opt_name} Accuracy')  # Plot accuracy over epochs\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8e606c",
   "metadata": {},
   "source": [
    "### Step 7: Evaluate the Final Model Performance\n",
    "\n",
    "**Explanation**: The best model (using Adam) is evaluated on the test set. A classification report provides precision, recall, and F1 scores, while a confusion matrix shows how well the model distinguishes between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25f3639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Evaluate the best model (e.g., Adam optimizer used for demonstration)\n",
    "best_model = create_model()\n",
    "best_model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "best_model.fit(X_train, y_train, epochs=50, batch_size=8, verbose=0)  # Retrain with Adam\n",
    "y_pred = np.argmax(best_model.predict(X_test), axis=1)  # Predicted classes\n",
    "y_true = np.argmax(y_test, axis=1)  # True classes\n",
    "\n",
    "# Classification report\n",
    "# Write your code here!\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "# Write your code here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf3f64d",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here to view/hide the solution.</summary>\n",
    "    \n",
    "```python\n",
    "    \n",
    "# Classification report\n",
    "print(classification_report(y_true, y_pred, target_names=iris['target_names']))  # Detailed metrics\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=iris['target_names'], yticklabels=iris['target_names'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "    \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095f7d14",
   "metadata": {},
   "source": [
    "The results and confusion matrix you provided for **7. Evaluate the Final Model Performance** indicate excellent performance, achieving 100% accuracy. Here's explanation of the Results:\n",
    "\n",
    "---\n",
    "\n",
    "### **Explanation of the Results:**\n",
    "\n",
    "1. **Classification Report:**\n",
    "   - **Precision**: Indicates the proportion of correct positive identifications. In this case, all predictions are 100% accurate for each class (Setosa, Versicolor, Virginica).  \n",
    "   - **Recall**: Indicates the proportion of actual positives that are correctly identified. Again, all predictions are perfect for all classes.  \n",
    "   - **F1-Score**: The harmonic mean of precision and recall, showing a balance between the two. Here, it's 1.00 for all classes, meaning the model has perfect classification performance.  \n",
    "   - **Support**: The number of actual instances for each class in the test set. The model evaluated 10 Setosa, 9 Versicolor, and 11 Virginica samples.\n",
    "\n",
    "2. **Confusion Matrix:**\n",
    "   - The matrix shows **no misclassifications**, as all samples are correctly predicted. Each diagonal entry represents correctly classified samples, while the off-diagonal entries (which are zero) represent errors.\n",
    "   - For example:\n",
    "     - 10 Setosa samples are correctly classified as Setosa.\n",
    "     - 9 Versicolor samples are correctly classified as Versicolor.\n",
    "     - 11 Virginica samples are correctly classified as Virginica.\n",
    "\n",
    "3. **Why 100% Accuracy?**\n",
    "   - **Dataset Simplicity**: The Iris dataset is small and well-separated in feature space, making it easier for even simple models to achieve perfect classification.  \n",
    "   - **Preprocessing**: Proper scaling of features and one-hot encoding of the target helped optimize the training process.  \n",
    "   - **Neural Network Architecture**: A well-chosen architecture with two hidden layers was sufficient for this dataset.  \n",
    "   - **Adam Optimizer**: Adaptive optimization likely played a role in fine-tuning weights effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### **You Should Understand:**\n",
    "- **Model Performance:** Achieving 100% accuracy is possible on smaller, clean datasets but unlikely for more complex, real-world datasets.  \n",
    "- **Importance of Preprocessing:** Proper preprocessing ensures the model receives data in a form that facilitates learning.  \n",
    "- **Confusion Matrix:** Always verify the distribution of predictions to ensure the model performs well across all classes, not just on accuracy alone.  \n",
    "- **Limitations:** While these results are impressive, real-world datasets often have noise, imbalance, and complexity that require more robust evaluation and regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849de31a",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "- Different optimizers (SGD, Momentum, Adam) impact convergence and accuracy differently.\n",
    "- Visualization of training progress aids in understanding model behavior.\n",
    "- Adam typically outperforms other optimizers for stability and speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e5fec0",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "\n",
    "This lab walked you through implementing a neural network, comparing optimization techniques, and evaluating model performance. Understanding how optimizers influence training is essential for building robust machine learning models. It prepares you to tackle more complex datasets and develop an intuition for selecting the right optimizer and preprocessing techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
