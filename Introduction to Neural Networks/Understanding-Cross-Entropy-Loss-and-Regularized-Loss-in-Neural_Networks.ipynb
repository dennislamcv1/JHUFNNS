{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2ad4f82",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"JHU.png\" width=\"200\" alt=\"Johns Hopkins University logo\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0415d39",
   "metadata": {},
   "source": [
    "# Hands-On Lab: Understanding Cross-Entropy Loss and Regularized Loss in Neural Networks\n",
    "\n",
    "Estimated time needed: **60** minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd5a77a",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "\n",
    "In this hands-on lab, you will explore essential loss functions used in neural networks for binary classification problems. Specifically, you will calculate the **Cross-Entropy Loss**, integrate **L2 Regularization**, and analyze how modifying predictions affects loss and model calibration.\n",
    "\n",
    "Loss functions play a vital role in guiding neural networks to learn optimal parameters. Regularization helps prevent overfitting, ensuring better generalization. Understanding these concepts is crucial for mastering neural network training.\n",
    "\n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "By the end of this lab, learners will be able to:\n",
    "\n",
    "- Compute the **Cross-Entropy Loss** for a binary classification task.\n",
    "- Integrate **L2 Regularization** with a loss function to compute the **Regularized Loss**.\n",
    "- Modify predictions to reduce loss and discuss the implications for calibration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12076a2a",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "In this lab, there is no external dataset. We will use a synthetic example:\n",
    "\n",
    "- **True Labels** (y_true): [1, 0, 1, 1, 0] Ground-truth binary labels.\n",
    "- **Predicted Probabilities** (y_pred): [0.9, 0.2, 0.8, 0.7, 0.1] Model-predicted probabilities for the positive class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cff208",
   "metadata": {},
   "source": [
    "## Assignment Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2010445b",
   "metadata": {},
   "source": [
    "### Step 1: Install and Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147b0e24",
   "metadata": {},
   "source": [
    "> **Note**: Please ignore any warnings that appear during execution; they will not affect the correctness of your code or the upcoming tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a52b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all necessary libraries\n",
    "!pip install numpy\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd26fa1",
   "metadata": {},
   "source": [
    "### Task 2: Compute Cross-Entropy Loss\n",
    "\n",
    "Cross-Entropy Loss measures the difference between the true labels and the predicted probabilities. For binary classification, it is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Cross-Entropy Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left( y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right)\n",
    "$$\n",
    "\n",
    "Here, \\( y_i \\) is the true label, and $\\hat{y}_i$ is the predicted probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7ad4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given data\n",
    "y_true = np.array([1, 0, 1, 1, 0])\n",
    "y_pred = np.array([0.9, 0.2, 0.8, 0.7, 0.1])\n",
    "\n",
    "# Cross-Entropy Loss function\n",
    "# Write your code here!\n",
    "\n",
    "\n",
    "\n",
    "# Calculate loss\n",
    "cross_entropy_loss = compute_cross_entropy_loss(y_true, y_pred)\n",
    "print(f\"Cross-Entropy Loss: {cross_entropy_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3c55f0",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here to view/hide the solution.</summary>\n",
    "    \n",
    "```python\n",
    "\n",
    "# Cross-Entropy Loss function\n",
    "def compute_cross_entropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-15  # To avoid log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip predictions\n",
    "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b416ce5",
   "metadata": {},
   "source": [
    "## Task 3: Compute Regularized Loss (L2 Regularization)\n",
    "\n",
    "To prevent overfitting, we add a regularization term to the loss function. Using L2 Regularization:\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Regularized Loss} = \\text{Cross-Entropy Loss} + \\lambda \\cdot \\frac{1}{2N} \\sum_{i=1}^{N} w_i^2\n",
    "$$\n",
    "\n",
    "\n",
    "Here, λ is the regularization strength, and \\( w_i \\) are model weights. \n",
    "\n",
    "For simplicity, assume `weights = [0.5, -0.3, 0.8, -0.2, 0.1]` and λ = 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b354af67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized Loss function\n",
    "# Write your code here!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Given weights and lambda\n",
    "weights = np.array([0.5, -0.3, 0.8, -0.2, 0.1])\n",
    "lambda_val = 0.5\n",
    "\n",
    "# Calculate regularized loss\n",
    "regularized_loss = compute_regularized_loss(y_true, y_pred, weights, lambda_val)\n",
    "print(f\"Regularized Loss: {regularized_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e062d3",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here to view/hide the solution.</summary>\n",
    "    \n",
    "```python\n",
    "# Regularized Loss function   \n",
    "def compute_regularized_loss(y_true, y_pred, weights, lambda_val):\n",
    "    cross_entropy = compute_cross_entropy_loss(y_true, y_pred)\n",
    "    l2_term = lambda_val * (np.sum(np.square(weights)) / (2 * len(weights)))\n",
    "    return cross_entropy + l2_term\n",
    "    \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bee64b",
   "metadata": {},
   "source": [
    "### Task 4: Modify Predictions and Discuss Calibration\n",
    "\n",
    "Modify y_pred to reduce the Cross-Entropy Loss. Observe how this impacts calibration the alignment between predicted probabilities and observed outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4427f64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified predictions (closer to true labels)\n",
    "y_pred_modified = np.array([1.0, 0.0, 1.0, 1.0, 0.0])  # Extreme calibration\n",
    "cross_entropy_loss_modified = compute_cross_entropy_loss(y_true, y_pred_modified)\n",
    "\n",
    "# Print the value of the modified cross-entropy loss, rounded to 4 decimal places.\n",
    "# Write your code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ea1c75",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here to view/hide the solution.</summary>\n",
    "    \n",
    "```python\n",
    "\n",
    "# Print the value of the modified cross-entropy loss, rounded to 4 decimal places.\n",
    "print(f\"Modified Cross-Entropy Loss: {cross_entropy_loss_modified:.4f}\")\n",
    "    \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111252cf",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "- **Before modification**: Predicted probabilities are calibrated but may result in higher loss.\n",
    "- **After modification**: Loss decreases, but predictions are no longer calibrated, making them overconfident."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095f7d14",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "- Cross-Entropy Loss quantifies the accuracy of predicted probabilities.\n",
    "- L2 Regularization penalizes large weights, helping prevent overfitting.\n",
    "- Overconfident predictions can reduce loss but may harm calibration, affecting real-world model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e5fec0",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "\n",
    "In this lab, you:\n",
    "\n",
    "- Computed **Cross-Entropy Loss** for binary classification.\n",
    "- Integrated **L2 Regularization** to calculate the **Regularized Loss**.\n",
    "- Modified predictions to explore the trade-offs between loss reduction and calibration.\n",
    "\n",
    "Understanding and balancing these metrics is essential for building robust and interpretable neural networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
